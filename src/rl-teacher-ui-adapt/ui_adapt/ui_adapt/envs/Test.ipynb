{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "import ui_adapt.utils as utils\n",
    "from ui_adapt.utils import Config\n",
    "from ui_adapt.envs.reward_predictor import RewardPredictor\n",
    "\n",
    "class UIAdaptationEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    _max_episode_steps = 20\n",
    "\n",
    "    def __init__(self, render_mode=None, \n",
    "                 config_data='config.json',\n",
    "                 initialState = None,\n",
    "                 ws_client= None, \n",
    "                 ws_server= None):\n",
    "        self.name = 'uiadaptation'\n",
    "        \n",
    "        self.config = Config(config_data=config_data)\n",
    "        self.reward_predictor = RewardPredictor(\"model.pckl\")\n",
    "\n",
    "        self.user = utils.get_random_user(self.config, userinfo=initialState)\n",
    "        self.uidesign = utils.get_random_ui(self.config, uiDesignInfo=initialState, client_ws=ws_client, server_ws=ws_server)\n",
    "        self.platform = utils.get_random_platform(self.config, platforminfo=initialState)\n",
    "        self.environment = utils.get_random_environment(self.config, envinfo=initialState)\n",
    "\n",
    "        self.state = self.get_observation()\n",
    "        self.actions = utils.get_actions(self.config)\n",
    "\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "\n",
    "        self.all_combinations = (self.uidesign.combinations + \n",
    "                            self.user.combinations +\n",
    "                           self.platform.combinations + \n",
    "                           self.environment.combinations)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete(self.all_combinations)\n",
    "        self.observation_space_size = np.prod(self.observation_space.nvec)\n",
    "        self.reward_collected = 0\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def get_image():\n",
    "        pass\n",
    "\n",
    "    def render(self, render_mode='human'):\n",
    "        '''\n",
    "        Returns: None\n",
    "        Show the current environment state e.g., the graphical window\n",
    "        This method must be implemented, but it's ok to have an empty implementation if\n",
    "        rendering is not important\n",
    "        '''\n",
    "        self.uidesign.render()\n",
    "        self.user.info()\n",
    "        self.environment.info()\n",
    "        self.platform.info()\n",
    "\n",
    "    def close (self):\n",
    "        '''\n",
    "        Return: None\n",
    "        This method is optional. Used to cleanup all resources (threads, GUI, etc)\n",
    "        '''\n",
    "        print(\"CLOSING\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    def step(self, action, verbose=False, sigma=0.5):\n",
    "        err_msg = f\"Action {action!r} ({type(action)}) invalid. Does not exist in ACTION_SPACE.\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"        \n",
    "\n",
    "        initial_state = self.get_observation()\n",
    "        done = False\n",
    "        info = {}\n",
    "        reward = 0\n",
    "        \n",
    "        penalize_flag = False\n",
    "\n",
    "        action_data = self.actions.get(action, {})\n",
    "        if not action_data:\n",
    "            print(f\"Action {action} is not defined.\")\n",
    "            return\n",
    "\n",
    "        name = action_data.get(\"name\", \"\")\n",
    "        target = action_data.get(\"target\", \"\")\n",
    "        value = action_data.get(\"value\", \"\")\n",
    "        api_call = action_data.get(\"api_call\", \"\")\n",
    "\n",
    "        if target == \"pass\":\n",
    "            # No operation\n",
    "            pass\n",
    "        else:\n",
    "            self.uidesign.update(target, value, api_call=api_call)\n",
    "            # if initial_state == self.get_observation():\n",
    "                # We took an action different than \"pass\" and nothing changed.\n",
    "                # penalize_flag = True\n",
    "\n",
    "        self.state = self.get_observation()\n",
    "        \n",
    "        alignment = self.reward_predictor.get_alignment(\n",
    "                    self.user, self.uidesign)\n",
    "        info[\"alignment\"] = alignment\n",
    "\n",
    "\n",
    "        '''if penalize_flag:\n",
    "            reward = -5\n",
    "        else:\n",
    "            reward = self.compute_reward()         \n",
    "            # reward = alignment\n",
    "        '''\n",
    "        reward = self.compute_reward(sigma=sigma)\n",
    "        \n",
    "        self.reward_collected += reward\n",
    "\n",
    "        \n",
    "        done = bool(reward>=0.7)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Performed action: {name}, Target: {target}, Value: {value}, Reward: {reward},\" \n",
    "                  f\"Collected Reward: {self.reward_collected}, Done: {done}\")\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        '''\n",
    "        We first clean everithing and then we create a new Context, app, etc\n",
    "        '''\n",
    "        # self.close()\n",
    "\n",
    "        self.user = utils.get_random_user(self.config)\n",
    "        self.platform = utils.get_random_platform(self.config)\n",
    "        self.environment = utils.get_random_environment(self.config)\n",
    "        self.uidesign = utils.get_random_ui(self.config)\n",
    "        self.state = self.get_observation()\n",
    "        self.reward_collected = 0\n",
    "        return self.state\n",
    "\n",
    "    def state_as_array(self, state, npArray=False):\n",
    "        state_array = []\n",
    "        for a in state:\n",
    "            if type(state[a]) is dict:\n",
    "                for b in state[a]:\n",
    "                    if type(state[a][b]) is dict:\n",
    "                        for c in state[a][b]:\n",
    "                            state_array.append(state[a][b][c])\n",
    "                    else:\n",
    "                        state_array.append(state[a][b])\n",
    "            else:\n",
    "                state_array.append(state[a])\n",
    "        if npArray:\n",
    "            return np.array(state_array)\n",
    "        return state_array\n",
    "    \n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "            This method traduces the representation of the state into an observation\n",
    "            that the gym can work with.\n",
    "        \"\"\"\n",
    "        uidesign_state = self.uidesign.get_state()\n",
    "        user_state = self.user.get_state()\n",
    "        environment_state = self.environment.get_state()\n",
    "        platform_state = self.platform.get_state()\n",
    "\n",
    "        self.state = {\n",
    "            **uidesign_state,\n",
    "            **user_state, \n",
    "            **platform_state,\n",
    "            **environment_state\n",
    "            }\n",
    "        return self.state_as_array(self.state, npArray=False)\n",
    "        \n",
    "    def compute_reward(self, sigma=1):\n",
    "        '''\n",
    "        Add here the reward function.\n",
    "        '''\n",
    "        # reward = (1 - sigma) * self.individual_reward() + sigma * self.general_reward()\n",
    "        reward = self.individual_reward()\n",
    "        return reward\n",
    "    \n",
    "    def general_reward(self):\n",
    "        return self.reward_predictor.predict(self.uidesign)[0]\n",
    "    \n",
    "    def individual_reward(self):\n",
    "        return self.reward_predictor.get_alignment(self.user, self.uidesign)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fd771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
